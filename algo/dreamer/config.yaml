---
env:
    name: LunarLander-v2
    video_path: video
    log_video: False
    n_envs: 1
    efficient_envvec: True
    seed: 0
model:
    encoder:
        has_cnn: True
    rssm:
        stoch_size: 64
        deter_size: 64
        hidden_size: 64
        activation: elu
    reward:
        units_list: [400, 400]
        activation: elu
    actor:
        units_list: [400, 400, 400, 400]
        activation: elu
        init_std: 5
        min_std: 1.e-4
    value:
        units_list: [400, 400, 400]
        activation: elu
    decoder:
        has_cnn: True
    temperature:
        temp_type: 'variable'
        value: .2
agent:
    algorithm: &algo dreamer
    polyak: .995
    
    # timer&interval
    MAX_STEPS: 2.e+7
    TIME_INTERVAL: 100
    LOG_INTERVAL: 100000
    timer: False

    # model
    free_nats: 3.
    kl_scale: 1.
    term_scale: 10.
    # behavior
    horizon: 15
    gamma: &gamma 0.99
    lambda: .95


    # model path: root_dir/model_name/models
    # tensorboard path: root_dir/model_name/logs
    # the following names are just examples; they will be reset in our training process
    root_dir: *algo                         # root path for tensorboard logs
    model_name: *algo

    loss_type: mse                          # huber or mse
    tbo: False                               # transformed Bellman Operator

    # arguments for optimizer
    optimizer: adam
    precision: 16
    clip_norm: 100
    weight_decay: 0
    model_lr: 1.e-3
    actor_lr: 1.e-3
    value_lr: 1.e-3
    epsilon: 1.e-8
    # clip_norm: 10
    
    # arguments for PER
    per_alpha: 0.5
    per_epsilon: 1.e-4

replay:
    type: uniform                      # proportional or uniform

    # arguments for general replay
    n_steps: 1
    gamma: *gamma
    batch_size: 512
    min_size: 5e4
    capacity: 1e6
