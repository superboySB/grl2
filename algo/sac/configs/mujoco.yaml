---
algorithm: &algo sac
name: sac
info: sac

precision: 32

# model path: root_dir/model_name/name
# tensorboard path: root_dir/model_name/logs
# the following names are just examples; they will be re-specified in the entry point
root_dir: *algo
model_name: *algo

routine:
  algorithm: *algo

  MAX_STEPS: 3e6
  n_steps: &nsteps 1
  LOG_PERIOD: 1e4
  EVAL_PERIOD: null

  n_eval_envs: 1000
  RECORD_VIDEO: False
  N_EVAL_EPISODES: 1
  size: [256, 256]

env:
  env_name: &env_name mujoco-Walker2d-v3
  n_runners: &nrunners 1
  n_envs: &nenvs 1
  max_episode_steps: 1000
  to_multi_agent: True
  timeout_done: &td False

agent: {}

monitor:
  use_tensorboard: True

strategy:
  algorithm: *algo
  train_loop: 
    n_epochs: 1

model:
  aid: 0
  gamma: &gamma .99
  polyak: .995
  n_Qs: 2

  policy:
    nn_id: policy
    units_list: [256, 256]
    w_init: orthogonal
    activation: relu
    norm: null
    rnn_type: &prnn null
    rnn_units: 64
    LOG_STD_MIN: -20
    LOG_STD_MAX: 2
    use_feature_norm: False
  Q:
    nn_id: Q
    units_list: [256, 256]
    w_init: orthogonal
    activation: relu
    norm: null
    rnn_type: &vrnn null
    rnn_units: 64
    use_feature_norm: False
  temp:
    nn_id: temp
    type: constant
    value: .2

loss:
  prnn_bptt: 10
  vrnn_bptt: 10

  stats:
    gamma: *gamma
    policy_coef: 1
    q_coef: 1
    temp_coef: 1

trainer:
  algorithm: *algo
  aid: 0

  policy_opt:
    opt_name: adam
    lr: 1e-3
    clip_norm: 10
    eps: 1e-5
  Q_opt:
    opt_name: adam
    lr: 1e-3
    clip_norm: 10
    eps: 1e-5
  temp_opt:
    opt_name: adam
    lr: 1e-3
    clip_norm: 10
    eps: 1e-5

actor: {}

buffer:
  type: uniform

  n_runners: *nrunners
  n_envs: *nenvs
  max_size: 1e6
  min_size: 1e4
  batch_size: 100
  sample_size: 1

  max_steps: 1
  gamma: *gamma

  n_steps: *nsteps
  sample_keys: &sk
    - obs
    - action
    - reward
    - discount
    - steps
