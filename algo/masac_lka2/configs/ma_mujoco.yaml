---
algorithm: &algo masac_lka2
name: masac_lka2
dynamics_name: dynamics
info: masac_lka2

precision: 32

# model path: root_dir/model_name/name
# tensorboard path: root_dir/model_name/logs
# the following names are just examples; they will be re-specified in the entry point
root_dir: *algo
model_name: *algo

routine:
    algorithm: *algo

    MAX_STEPS: 2e6
    n_steps: &nsteps 10
    LOG_PERIOD: 2e4
    EVAL_PERIOD: null

    n_eval_envs: 100
    RECORD_VIDEO: False
    N_EVAL_EPISODES: 1
    size: [256, 256]

    n_lookahead_steps: 1
    n_simulated_envs: &nse 2000
    n_simulated_steps: &nss 1
    lookahead_rollout: sim
    lookahead_replay: secondary
    quantify_model_errors: True

env:
    env_name: &env_name ma_mujoco-HalfCheetah_2x3
    n_runners: &nrunners 1
    n_envs: &nenvs 40
    max_episode_steps: 1000
    timeout_done: &td False
    env_args:
        agent_obsk: 0
    single_agent: False
    norm_obs: False

agent: {}

monitor:
    use_tensorboard: True

strategy:
    algorithm: *algo
    train_loop: 
        n_epochs: 400
        n_lka_epochs: 100

model:
    aid: 0
    joint_log_prob: True
    gamma: &gamma .99
    polyak: .995
    n_Qs: 2

    policy:
        nn_id: policy
        units_list: [256, 256]
        w_init: orthogonal
        activation: relu
        norm: null
        rnn_type: &prnn null
        rnn_units: 64
        LOG_STD_MIN: -20
        LOG_STD_MAX: 2
        use_feature_norm: False
    Q:
        nn_id: Q
        units_list: [256, 256]
        w_init: orthogonal
        activation: relu
        norm: null
        rnn_type: &vrnn null
        rnn_units: 64
        use_feature_norm: False
    temp:
        nn_id: temp
        type: variable
        value: .2

loss:
    prnn_bptt: 10
    vrnn_bptt: 10

    stats:
        gamma: *gamma
        policy_coef: 1
        q_coef: 1
        temp_coef: 1

trainer:
    algorithm: *algo
    aid: 0

    policy_opt:
        opt_name: adam
        lr: 3e-4
        clip_norm: 10
        eps: 1e-5
    Q_opt:
        opt_name: adam
        lr: 3e-4
        clip_norm: 10
        eps: 1e-5
    temp_opt:
        opt_name: adam
        lr: 3e-4
        clip_norm: 10
        eps: 1e-5

actor: {}

buffer:
    type: dual

    recent_primal_replay: False
    detached: True

    n_runners: *nrunners
    n_envs: *nenvs
    max_size: 1e5
    min_size: 1e3
    batch_size: 256
    sample_size: 1

    max_steps: 1
    gamma: *gamma

    primal_percentage: .5
    n_steps: *nsteps
    sample_keys: &sk
        - obs
        - action
        - reward
        - discount
        - steps

    primal_replay:
        type: uniform

        n_runners: *nrunners
        n_envs: *nenvs
        max_size: 1e5
        min_size: 1e3
        sample_size: 1

        max_steps: 1
        gamma: *gamma

        sample_keys: *sk

    secondary_replay:
        type: uniform

        n_runners: *nrunners
        n_envs: *nenvs
        max_size: 1e5
        min_size: 1e3
        sample_size: 1

        max_steps: 1
        gamma: *gamma

        sample_keys: *sk
