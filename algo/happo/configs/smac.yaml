---
algorithm: &algo happo
name: happo
info: happo

precision: 32

# model path: root_dir/model_name/name
# tensorboard path: root_dir/model_name/logs
# the following names are just examples; they will be re-specified in the entry point
root_dir: *algo
model_name: *algo

routine:
  algorithm: *algo

  MAX_STEPS: 1e7
  n_steps: &nsteps 100
  LOG_PERIOD: 2e5
  EVAL_PERIOD: null

  RECORD_VIDEO: False
  N_EVAL_EPISODES: 1
  size: [256, 256]

  n_lka_steps: 0
  lookahead_rollout: sim
  compute_return_at_once: False
  ignore_ratio_for_ego: False
  ignore_ratio_for_lookahead: False
  perm: null

env:
  env_name: &env_name smac-5m_vs_6m
  n_runners: &nrunners 1
  n_envs: &nenvs 32
  timeout_done: &td False
  env_args:
    obs_all_health: True
    obs_own_health: True
    obs_last_action: True   # Different from PyMARL, we decide it to be True by default here.
    obs_pathing_grid: False
    obs_terrain_height: False
    obs_instead_of_state: False
    obs_timestep_number: False
    obs_agent_id: True  # We align it to the environment in HAPPO or MAPPO
    state_pathing_grid: False
    state_terrain_height: False
    state_last_action: True
    state_timestep_number: False
    state_agent_id: True # We align it to the environment in HAPPO or MAPPO
    use_state_agent: True
    add_center_xy: True
    add_distance_state: False
    add_xy_state: False
    add_visible_state: False
    add_enemy_action_state: False
    use_sample_mask: True
    use_action_mask: True
    sample_mask_at_done: 0
  uid2aid: [0, 1, 2, 3, 4]

agent: {}

monitor:
  use_tensorboard: True

strategy:
  algorithm: *algo
  train_loop: {}

model:
  aid: 0
  joint_log_prob: False
  gamma: &gamma .99

  policy:
    nn_id: policy
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: layer
    out_scale: .01
    rnn_type: &prnn null
    rnn_units: 64
    init_std: .2
    sigmoid_scale: True
    std_x_coef: 1.
    std_y_coef: .5
    use_feature_norm: False
  value:
    nn_id: value
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: layer
    rnn_type: &vrnn null
    rnn_units: 64
    use_feature_norm: False

loss:
  # hyperparams for value target and advantage
  target_type: vtrace
  c_clip: 1
  rho_clip: 1
  adv_type: gae
  norm_adv: True
  popart: &popart True

  prnn_bptt: 10
  vrnn_bptt: 10

  # hyperparams for policy optimization
  pg_type: ppo
  ppo_clip_range: .2
  reg_type: kl_reverse
  reg_coef: 0
  pos_reg_coef: 0
  neg_reg_coef: 0
  policy_sample_mask: True

  # hyperparams for value learning
  value_loss: clip_huber
  huber_threshold: 10
  value_clip_range: .2
  value_sample_mask: True

  stats:
    gamma: *gamma
    lam: &lam .95
    pg_coef: 1
    entropy_coef: .01
    value_coef: 1

trainer:
  algorithm: *algo
  aid: 0
  n_runners: *nrunners
  n_envs: *nenvs
  n_epochs: 10
  n_lka_epochs: 10
  n_mbs: &nmbs 1
  n_steps: *nsteps     # BPTT length

  popart: *popart

  policy_opt:
    opt_name: adam
    lr: 3e-4
    clip_norm: 10
    eps: 1e-5
  value_opt:
    opt_name: adam
    lr: 3e-4
    clip_norm: 10
    eps: 1e-5

actor:
  update_obs_rms_at_execution: False
  update_obs_at_execution: False
  rms:
    obs_names: [obs, global_state]
    obs_normalized_axis: [0, 1, 2]
    reward_normalized_axis: [0, 1, 2]
    normalize_obs: False
    obs_clip: 10
    normalize_reward: False
    update_reward_rms_in_time: False
    gamma: *gamma

buffer:
  type: ac

  n_runners: *nrunners
  n_envs: *nenvs
  n_steps: *nsteps
  queue_size: 2
  timeout_done: *td

  gamma: *gamma
  lam: *lam

  sample_keys: 
    - obs
    - action
    - value
    - reward
    - discount
    - reset
    - mu_logprob
    - mu_logits
    - state_reset
    - state
